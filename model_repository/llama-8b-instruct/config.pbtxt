name: "llama-8b-instruct"
backend: "vllm"

model_transaction_policy {
  decoupled: True
}

instance_group [
  {
    count: 1
    kind: KIND_GPU
  }
]